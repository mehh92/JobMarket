# üöÄ Guide de Configuration Apache Airflow

Ce guide vous explique comment configurer et utiliser Apache Airflow pour orchestrer le pipeline ETL JobMarket.

---

## üìã Table des mati√®res

1. [Vue d'ensemble](#vue-densemble)
2. [Pr√©requis](#pr√©requis)
3. [Installation et D√©marrage](#installation-et-d√©marrage)
4. [Configuration de la connexion PostgreSQL](#configuration-de-la-connexion-postgresql)
5. [Utilisation du DAG](#utilisation-du-dag)
6. [Architecture du Pipeline](#architecture-du-pipeline)
7. [D√©pannage](#d√©pannage)

---

## üéØ Vue d'ensemble

**Airflow** est un orchestrateur de workflows qui va g√©rer automatiquement l'ex√©cution de notre pipeline ETL dans le bon ordre :

```
Scraping Adzuna ‚Üí Chargement RAW ‚Üí Transformation STAGING ‚Üí Transformation ANALYTICS ‚Üí V√©rification
```

### Avantages :
- ‚úÖ **Automatisation compl√®te** du pipeline
- ‚úÖ **Interface web** pour suivre l'ex√©cution
- ‚úÖ **Retry automatique** en cas d'erreur
- ‚úÖ **Logs d√©taill√©s** pour chaque t√¢che
- ‚úÖ **Planification** (quotidienne, hebdomadaire, etc.)
- ‚úÖ **Gestion des d√©pendances** entre t√¢ches

---

## üîß Pr√©requis

### 1. Docker Desktop install√© et d√©marr√©
V√©rifiez que Docker est bien lanc√© :
```bash
docker --version
docker ps
```

### 2. PostgreSQL d√©marr√© (via docker-compose)
```bash
docker-compose up -d postgres
docker ps  # V√©rifier que jobmarket_postgres est "Up (healthy)"
```

### 3. Configuration Adzuna API
Assurez-vous que `src/config.json` existe et contient vos cl√©s API :
```json
{
  "adzuna": {
    "app_id": "VOTRE_APP_ID",
    "app_key": "VOTRE_APP_KEY"
  }
}
```

---

## üöÄ Installation et D√©marrage

### √âtape 1 : D√©marrer tous les services Docker

```bash
# Depuis le r√©pertoire racine du projet
cd C:\Users\dubas\Documents\JobMarket

# D√©marrer PostgreSQL + Airflow
docker-compose up -d

# V√©rifier que tout fonctionne
docker-compose ps
```

**R√©sultat attendu :**
```
NAME                    STATUS
jobmarket_airflow      Up (healthy)
jobmarket_postgres     Up (healthy)
```

### √âtape 2 : Attendre l'initialisation d'Airflow

La premi√®re fois, Airflow doit initialiser sa base de donn√©es. Cela peut prendre 1-2 minutes.

**Suivre les logs :**
```bash
docker-compose logs -f airflow
```

**Attendez de voir :**
```
[...] Starting the scheduler...
[...] Starting the web server...
```

Vous pouvez arr√™ter le suivi des logs avec `Ctrl+C`.

### √âtape 3 : Acc√©der √† l'interface web Airflow

Ouvrez votre navigateur et allez sur :
```
http://localhost:8080
```

**Identifiants par d√©faut :**
- **Username** : `admin`
- **Password** : `admin`

---

## üîó Configuration de la connexion PostgreSQL

Pour que le DAG puisse ex√©cuter les transformations SQL, il faut cr√©er une connexion PostgreSQL dans Airflow.

### Option A : Via l'interface web (Recommand√©)

1. **Connectez-vous** √† Airflow : http://localhost:8080
2. Allez dans **Admin ‚Üí Connections**
3. Cliquez sur **"+"** (Ajouter une connexion)
4. Remplissez les champs :

| Champ             | Valeur                    |
|-------------------|---------------------------|
| **Connection Id** | `jobmarket_postgres`      |
| **Connection Type** | `Postgres`              |
| **Host**          | `postgres`                |
| **Schema**        | `jobmarket`               |
| **Login**         | `jobmarket_user`          |
| **Password**      | `jobmarket_pass`          |
| **Port**          | `5432`                    |

5. Cliquez sur **Save**

### Option B : Via CLI (dans le conteneur)

```bash
docker exec -it jobmarket_airflow bash

airflow connections add 'jobmarket_postgres' \
    --conn-type 'postgres' \
    --conn-host 'postgres' \
    --conn-schema 'jobmarket' \
    --conn-login 'jobmarket_user' \
    --conn-password 'jobmarket_pass' \
    --conn-port 5432

exit
```

---

## ‚ñ∂Ô∏è Utilisation du DAG

### 1. V√©rifier que le DAG est visible

1. Allez sur http://localhost:8080
2. Vous devriez voir le DAG : **`jobmarket_etl_pipeline`**
3. Il est en **pause** par d√©faut (interrupteur gris)

### 2. Activer le DAG

Cliquez sur l'**interrupteur** √† gauche du nom du DAG pour le passer en bleu (activ√©).

### 3. Lancer le DAG manuellement

1. Cliquez sur le **nom du DAG** : `jobmarket_etl_pipeline`
2. En haut √† droite, cliquez sur le bouton **"‚ñ∂Ô∏è Trigger DAG"**
3. Cliquez sur **"Trigger"** dans la popup

### 4. Suivre l'ex√©cution

#### Vue graphique
Cliquez sur l'onglet **"Graph"** pour voir le pipeline :

```
start_pipeline ‚Üí scrape_adzuna ‚Üí load_to_postgres ‚Üí transform_to_staging 
                                                    ‚Üì
                                            transform_to_analytics
                                                    ‚Üì
                                             verify_pipeline ‚Üí end_pipeline
```

- üü¢ **Vert** = T√¢che r√©ussie
- üîµ **Bleu** = T√¢che en cours
- üî¥ **Rouge** = T√¢che √©chou√©e
- ‚ö™ **Blanc** = T√¢che en attente

#### Logs d'une t√¢che
1. Cliquez sur une t√¢che (ex: `scrape_adzuna`)
2. Cliquez sur **"Log"**
3. Vous verrez les logs en temps r√©el

### 5. Planification automatique (optionnel)

Pour ex√©cuter le pipeline **automatiquement tous les jours √† 6h du matin**, modifiez le DAG :

```python
# Dans dags/jobmarket_etl_pipeline.py
schedule_interval='0 6 * * *',  # Cron : tous les jours √† 6h
```

Red√©marrez Airflow :
```bash
docker-compose restart airflow
```

---

## üèóÔ∏è Architecture du Pipeline

### Fichiers cr√©√©s

```
JobMarket/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ jobmarket_etl_pipeline.py    # DAG Airflow principal
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ scraper_adzuna.py           # Scraper Adzuna
‚îÇ   ‚îú‚îÄ‚îÄ db_loader.py                # Chargeur PostgreSQL
‚îÇ   ‚îú‚îÄ‚îÄ db_config.py                # Config DB centralis√©e
‚îÇ   ‚îî‚îÄ‚îÄ config.json                 # Cl√©s API (GIT IGNOR√â)
‚îú‚îÄ‚îÄ sql/
‚îÇ   ‚îú‚îÄ‚îÄ init/                       # Scripts d'initialisation DB
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01_create_schemas.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 02_create_raw_tables.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 03_create_staging_tables.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 04_create_analytics_tables.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 05_create_views.sql
‚îÇ   ‚îî‚îÄ‚îÄ transformations/            # Scripts de transformation
‚îÇ       ‚îú‚îÄ‚îÄ 01_load_staging.sql     # RAW ‚Üí STAGING
‚îÇ       ‚îú‚îÄ‚îÄ 02_load_analytics.sql   # STAGING ‚Üí ANALYTICS
‚îÇ       ‚îî‚îÄ‚îÄ 03_refresh_all.sql      # Refresh complet
‚îú‚îÄ‚îÄ data/                           # Donn√©es JSON (GIT IGNOR√â)
‚îú‚îÄ‚îÄ logs/                           # Logs Airflow
‚îî‚îÄ‚îÄ docker-compose.yml              # Config Docker
```

### Sch√©ma du Pipeline

```mermaid
graph LR
    A[Adzuna API] -->|Scrape| B[JSON File]
    B -->|Load| C[raw.jobs_raw]
    C -->|Transform| D[staging.jobs_flattened]
    D -->|Enrich| E[analytics.jobs_clean]
    E -->|Views| F[Vues Analytics]
    F -->|Connect| G[DBeaver / BI Tools]
```

### D√©tail des t√¢ches du DAG

| T√¢che                   | Description                                | Dur√©e estim√©e |
|-------------------------|--------------------------------------------|---------------|
| `start_pipeline`        | Marqueur de d√©marrage                      | < 1s          |
| `scrape_adzuna`         | R√©cup√®re les offres depuis Adzuna          | 5-15 min      |
| `load_to_postgres`      | Charge le JSON dans `raw.jobs_raw`         | 10-30s        |
| `transform_to_staging`  | Aplatit les donn√©es en colonnes SQL       | 5-10s         |
| `transform_to_analytics`| Enrichit avec calculs et flags            | 5-10s         |
| `verify_pipeline`       | V√©rifie les donn√©es et affiche les stats  | 2-5s          |
| `end_pipeline`          | Marqueur de fin                            | < 1s          |

---

## üîç V√©rification des r√©sultats

### 1. Dans Airflow
V√©rifiez les logs de la t√¢che `verify_pipeline` pour voir les statistiques :
```
üìä STATISTIQUES DU PIPELINE:
   üîç Offres scrap√©es: 14000
   üì• Offres ins√©r√©es (RAW): 14000
   üóÉÔ∏è  Import ID: 1

üìã TABLES POSTGRESQL:
   ‚Ä¢ raw.jobs_raw: 14000 lignes
   ‚Ä¢ staging.jobs_flattened: 14000 lignes
   ‚Ä¢ analytics.jobs_clean: 14000 lignes

‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS! üéâ
```

### 2. Dans DBeaver
Connectez-vous √† PostgreSQL et ex√©cutez :

```sql
-- Compter les lignes
SELECT COUNT(*) FROM raw.jobs_raw;
SELECT COUNT(*) FROM staging.jobs_flattened;
SELECT COUNT(*) FROM analytics.jobs_clean;

-- V√©rifier les vues analytics
SELECT * FROM analytics.vw_salaries_by_job LIMIT 10;
SELECT * FROM analytics.vw_top_companies LIMIT 10;
SELECT * FROM analytics.vw_geo_distribution LIMIT 10;
```

---

## üõ†Ô∏è D√©pannage

### Probl√®me : Airflow ne d√©marre pas

**Sympt√¥me :**
```bash
docker-compose ps
# jobmarket_airflow est "Restarting"
```

**Solution :**
```bash
# Voir les logs
docker-compose logs airflow

# Souvent, c'est un probl√®me de m√©moire. Augmentez la RAM de Docker (4GB minimum)
# Settings ‚Üí Resources ‚Üí Memory ‚Üí 4GB

# Red√©marrer
docker-compose restart airflow
```

### Probl√®me : DAG n'appara√Æt pas dans l'interface

**Solutions :**
1. V√©rifier que le fichier est bien dans `dags/`
2. V√©rifier la syntaxe Python :
   ```bash
   docker exec -it jobmarket_airflow bash
   python /opt/airflow/dags/jobmarket_etl_pipeline.py
   exit
   ```
3. Forcer le refresh :
   ```bash
   docker-compose restart airflow
   ```

### Probl√®me : Erreur `Connection 'jobmarket_postgres' not found`

**Solution :**
La connexion PostgreSQL n'est pas configur√©e. Suivez la section [Configuration de la connexion PostgreSQL](#configuration-de-la-connexion-postgresql).

### Probl√®me : Erreur `No module named 'scraper_adzuna'`

**Solution :**
Le dossier `src/` n'est pas mont√© correctement. V√©rifiez `docker-compose.yml` :
```yaml
volumes:
  - ./src:/opt/airflow/src
```

Red√©marrez :
```bash
docker-compose down
docker-compose up -d
```

### Probl√®me : Scraping √©choue (cl√©s API invalides)

**Solution :**
V√©rifiez que `src/config.json` contient les bonnes cl√©s :
```bash
cat src/config.json
```

Si vide ou incorrect, mettez √† jour et red√©marrez :
```bash
docker-compose restart airflow
```

### Probl√®me : Impossible de se connecter √† PostgreSQL depuis Airflow

**Solution :**
V√©rifiez que PostgreSQL est bien d√©marr√© :
```bash
docker ps
# jobmarket_postgres doit √™tre "Up (healthy)"

# Tester la connexion depuis le conteneur Airflow
docker exec -it jobmarket_airflow bash
psql -h postgres -U jobmarket_user -d jobmarket -c "SELECT 1;"
# Mot de passe : jobmarket_pass
exit
```

---

## üìä Prochaines √©tapes

1. ‚úÖ **Pipeline fonctionnel** : Vous avez un ETL automatis√© de bout en bout
2. üîÑ **Planification** : Activez le `schedule_interval` pour automatiser quotidiennement
3. üìà **Visualisation** : Connectez DBeaver ou un outil BI (Metabase, Superset, Power BI) aux vues analytics
4. üß™ **Tests** : Ajoutez des tests de qualit√© de donn√©es (pytest + Great Expectations)
5. üìß **Alertes** : Configurez les emails pour √™tre notifi√© en cas d'√©chec

---

## üìö Ressources

- [Documentation Apache Airflow](https://airflow.apache.org/docs/)
- [PostgreSQL Documentation](https://www.postgresql.org/docs/)
- [Guide DBeaver](./DBEAVER_SETUP.md)
- [Guide PostgreSQL](./DATABASE_SETUP.md)

---

**F√©licitations ! Votre pipeline ETL avec Airflow est op√©rationnel !** üéâ

Pour toute question, consultez les logs ou ouvrez une issue sur le repository.

