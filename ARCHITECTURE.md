# ğŸ—ï¸ Architecture du Projet JobMarket

Ce document prÃ©sente l'architecture complÃ¨te du projet avec des schÃ©mas visuels.

---

## ğŸ“Š Vue d'ensemble - Infrastructure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DOCKER COMPOSE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚               ğŸ˜ PostgreSQL 16                         â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  Port: 5432                                            â”‚   â”‚
â”‚  â”‚  Volume: postgres_data (persistant)                    â”‚   â”‚
â”‚  â”‚                                                        â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚   â”‚
â”‚  â”‚  â”‚ Base:       â”‚  â”‚ Base:       â”‚                    â”‚   â”‚
â”‚  â”‚  â”‚ airflow     â”‚  â”‚ jobmarket   â”‚                    â”‚   â”‚
â”‚  â”‚  â”‚             â”‚  â”‚             â”‚                    â”‚   â”‚
â”‚  â”‚  â”‚ Owner:      â”‚  â”‚ Schemas:    â”‚                    â”‚   â”‚
â”‚  â”‚  â”‚ airflow     â”‚  â”‚ â€¢ raw       â”‚                    â”‚   â”‚
â”‚  â”‚  â”‚             â”‚  â”‚ â€¢ staging   â”‚                    â”‚   â”‚
â”‚  â”‚  â”‚ (MÃ©tastore) â”‚  â”‚ â€¢ analytics â”‚                    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†•                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚            ğŸš€ Apache Airflow 2.10                      â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  Port: 8080 (Interface Web)                            â”‚   â”‚
â”‚  â”‚  Executor: LocalExecutor                               â”‚   â”‚
â”‚  â”‚  Volume: airflow_data (persistant)                     â”‚   â”‚
â”‚  â”‚                                                        â”‚   â”‚
â”‚  â”‚  Composants:                                           â”‚   â”‚
â”‚  â”‚  â€¢ Webserver  â†’ Interface utilisateur                 â”‚   â”‚
â”‚  â”‚  â€¢ Scheduler  â†’ Orchestration des DAGs                â”‚   â”‚
â”‚  â”‚  â€¢ Database   â†’ MÃ©tastore (dans PostgreSQL)           â”‚   â”‚
â”‚  â”‚                                                        â”‚   â”‚
â”‚  â”‚  Volumes montÃ©s:                                       â”‚   â”‚
â”‚  â”‚  â€¢ ./dags     â†’ /opt/airflow/dags                     â”‚   â”‚
â”‚  â”‚  â€¢ ./src      â†’ /opt/airflow/src                      â”‚   â”‚
â”‚  â”‚  â€¢ ./sql      â†’ /opt/airflow/sql                      â”‚   â”‚
â”‚  â”‚  â€¢ ./data     â†’ /opt/airflow/data                     â”‚   â”‚
â”‚  â”‚  â€¢ ./logs     â†’ /opt/airflow/logs                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              ğŸŒ Network: jobmarket_network             â”‚   â”‚
â”‚  â”‚  (Bridge - Communication entre conteneurs)             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†•
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ DBeaver â”‚  (Client SQL externe - Port 5432)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Flux de donnÃ©es - Pipeline ETL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PIPELINE ETL COMPLET                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1ï¸âƒ£ EXTRACTION (Scraping)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Adzuna API  â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚ HTTP Requests
          â”‚ (max 700 pages)
          â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ scraper_adzuna.pyâ”‚ â† Python
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ Sauvegarde
          â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ jobs_data.json   â”‚ â† Fichier temporaire
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   (data/ - ignorÃ© Git)
          â”‚
          â†“

2ï¸âƒ£ CHARGEMENT (Load to DB)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  db_loader.py    â”‚ â† Python
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ INSERT batch (1000 lignes)
          â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  PostgreSQL - Schema: raw               â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  raw.jobs_raw                           â”‚
   â”‚  â€¢ id (PK)                              â”‚
   â”‚  â€¢ job_id (unique)                      â”‚
   â”‚  â€¢ data (JSONB) â† JSON complet          â”‚
   â”‚  â€¢ source ('adzuna')                    â”‚
   â”‚  â€¢ created_at / updated_at              â”‚
   â”‚                                         â”‚
   â”‚  raw.import_metadata                    â”‚
   â”‚  â€¢ id, search_term, total_jobs          â”‚
   â”‚  â€¢ scraping_date, api_source            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â†“

3ï¸âƒ£ TRANSFORMATION 1 (Staging)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 01_load_staging.sql              â”‚ â† SQL
   â”‚ (Extraction JSON â†’ Colonnes SQL) â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ INSERT/UPDATE
              â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  PostgreSQL - Schema: staging           â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  staging.jobs_flattened                 â”‚
   â”‚  â€¢ job_id (PK)                          â”‚
   â”‚  â€¢ title, description, created          â”‚
   â”‚  â€¢ contract_type, contract_time         â”‚
   â”‚  â€¢ salary_min, salary_max               â”‚
   â”‚  â€¢ latitude, longitude                  â”‚
   â”‚  â€¢ location_display, city, region       â”‚
   â”‚  â€¢ company_name, category_label         â”‚
   â”‚  â€¢ redirect_url                         â”‚
   â”‚  â€¢ raw_id (FK), processed_at            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â†“

4ï¸âƒ£ TRANSFORMATION 2 (Analytics)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 02_load_analytics.sql            â”‚ â† SQL
   â”‚ (Enrichissement + Calculs)       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ INSERT/UPDATE
              â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  PostgreSQL - Schema: analytics         â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  analytics.jobs_clean                   â”‚
   â”‚  â€¢ job_id (PK)                          â”‚
   â”‚  â€¢ [Toutes colonnes staging]            â”‚
   â”‚  â€¢ salary_avg, salary_avg_k (calculs)   â”‚
   â”‚  â€¢ is_paris, is_ile_de_france (flags)   â”‚
   â”‚  â€¢ is_data_scientist, is_data_analyst   â”‚
   â”‚  â€¢ is_data_engineer, is_alternance      â”‚
   â”‚  â€¢ year, month, year_month              â”‚
   â”‚  â€¢ description_length                   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â†“

5ï¸âƒ£ VUES ANALYTIQUES (Business Intelligence)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Vues SQL (05_create_views.sql)         â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  ğŸ“Š vw_salaries_by_job                  â”‚
   â”‚     â†’ Salaires moyens par type de poste â”‚
   â”‚                                         â”‚
   â”‚  ğŸ¢ vw_top_companies                    â”‚
   â”‚     â†’ Entreprises qui recrutent le +    â”‚
   â”‚                                         â”‚
   â”‚  ğŸ—ºï¸ vw_geo_distribution                 â”‚
   â”‚     â†’ RÃ©partition gÃ©ographique          â”‚
   â”‚                                         â”‚
   â”‚  ğŸ“ˆ vw_monthly_trends                   â”‚
   â”‚     â†’ Tendances mensuelles              â”‚
   â”‚                                         â”‚
   â”‚  ğŸ™ï¸ vw_top_cities                       â”‚
   â”‚     â†’ Top villes par nombre d'offres    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   DBeaver    â”‚  RequÃªtes SQL / Visualisations
          â”‚   Metabase   â”‚  Dashboards interactifs
          â”‚   Superset   â”‚  Analyses avancÃ©es
          â”‚   Power BI   â”‚  Rapports business
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Architecture du DAG Airflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           DAG: jobmarket_etl_pipeline                          â”‚
â”‚           Schedule: Manuel (ou @daily en production)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  start_pipeline â”‚  (DummyOperator)
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  scrape_adzuna                           â”‚  PythonOperator
     â”‚  â€¢ Appelle scraper_adzuna.py             â”‚  DurÃ©e: 1-30 min
     â”‚  â€¢ RÃ©cupÃ¨re donnÃ©es depuis Adzuna API    â”‚  (selon mode)
     â”‚  â€¢ Sauvegarde dans jobs_data.json        â”‚
     â”‚  â€¢ XCom: filepath, nb_jobs               â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  load_to_postgres                        â”‚  PythonOperator
     â”‚  â€¢ Appelle db_loader.py                  â”‚  DurÃ©e: 10-30s
     â”‚  â€¢ Charge JSON â†’ raw.jobs_raw            â”‚
     â”‚  â€¢ Insert mÃ©tadonnÃ©es                    â”‚
     â”‚  â€¢ XCom: import_id, nb_jobs_inserted     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  transform_to_staging                    â”‚  PostgresOperator
     â”‚  â€¢ ExÃ©cute 01_load_staging.sql           â”‚  DurÃ©e: 5-10s
     â”‚  â€¢ RAW â†’ STAGING (aplatissement JSON)    â”‚
     â”‚  â€¢ Extraction colonnes                   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  transform_to_analytics                  â”‚  PostgresOperator
     â”‚  â€¢ ExÃ©cute 02_load_analytics.sql         â”‚  DurÃ©e: 5-10s
     â”‚  â€¢ STAGING â†’ ANALYTICS (enrichissement)  â”‚
     â”‚  â€¢ Calculs + Flags                       â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  verify_pipeline                         â”‚  PythonOperator
     â”‚  â€¢ Compte lignes dans chaque table       â”‚  DurÃ©e: 2-5s
     â”‚  â€¢ Affiche statistiques                  â”‚
     â”‚  â€¢ VÃ©rifie intÃ©gritÃ©                     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  end_pipeline   â”‚  (DummyOperator)
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Variables Airflow (configuration):
  â€¢ scraping_mode: "test" (5 pages) ou "production" (700 pages)
  â€¢ search_term: "data"
  â€¢ test_max_pages: 5
  â€¢ max_pages: 700
  â€¢ delay: 0.2
```

---

## ğŸ“ Structure des fichiers du projet

```
JobMarket/
â”‚
â”œâ”€â”€ ğŸ“„ Configuration & Documentation
â”‚   â”œâ”€â”€ README.md                    # Vue d'ensemble
â”‚   â”œâ”€â”€ ARCHITECTURE.md              # Ce fichier (architecture)
â”‚   â”œâ”€â”€ DECISIONS.md                 # Choix techniques
â”‚   â”œâ”€â”€ .gitignore                  # Exclusions Git
â”‚   â”œâ”€â”€ requirements.txt            # DÃ©pendances Python
â”‚   â””â”€â”€ docker-compose.yml          # Infrastructure Docker
â”‚
â”œâ”€â”€ ğŸ“š docs/                        # Documentation dÃ©taillÃ©e
â”‚   â”œâ”€â”€ AIRFLOW_SETUP.md            # Guide Airflow
â”‚   â”œâ”€â”€ AIRFLOW_VARIABLES.md        # Config modes TEST/PROD
â”‚   â”œâ”€â”€ DATABASE_SETUP.md           # Guide PostgreSQL
â”‚   â””â”€â”€ DBEAVER_SETUP.md            # Guide DBeaver
â”‚
â”œâ”€â”€ ğŸ”„ dags/                        # DAGs Airflow
â”‚   â””â”€â”€ jobmarket_etl_pipeline.py   # Pipeline ETL principal
â”‚
â”œâ”€â”€ ğŸ src/                         # Code source Python
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.json                 # ClÃ©s API (non versionnÃ©)
â”‚   â”œâ”€â”€ config.example.adzuna.json  # Template config
â”‚   â”œâ”€â”€ scraper_adzuna.py           # Scraper Adzuna API
â”‚   â”œâ”€â”€ db_config.py                # Config PostgreSQL
â”‚   â”œâ”€â”€ db_loader.py                # Chargeur PostgreSQL
â”‚   â””â”€â”€ README.md                   # Doc du code source
â”‚
â”œâ”€â”€ ğŸ—„ï¸ sql/                         # Scripts SQL
â”‚   â”œâ”€â”€ init/                       # Initialisation (auto-exec)
â”‚   â”‚   â”œâ”€â”€ 00_create_airflow_db.sql    # Base Airflow
â”‚   â”‚   â”œâ”€â”€ 01_create_schemas.sql       # raw, staging, analytics
â”‚   â”‚   â”œâ”€â”€ 02_create_raw_tables.sql    # Tables RAW
â”‚   â”‚   â”œâ”€â”€ 03_create_staging_tables.sql # Tables STAGING
â”‚   â”‚   â”œâ”€â”€ 04_create_analytics_tables.sql # Tables ANALYTICS
â”‚   â”‚   â””â”€â”€ 05_create_views.sql         # Vues analytiques
â”‚   â”‚
â”‚   â””â”€â”€ transformations/            # Transformations ETL
â”‚       â”œâ”€â”€ 01_load_staging.sql     # RAW â†’ STAGING
â”‚       â”œâ”€â”€ 02_load_analytics.sql   # STAGING â†’ ANALYTICS
â”‚       â””â”€â”€ 03_refresh_all.sql      # Refresh complet
â”‚
â”œâ”€â”€ ğŸ“Š data/                        # DonnÃ©es (ignorÃ© Git)
â”‚   â”œâ”€â”€ .gitkeep
â”‚   â””â”€â”€ jobs_data.json              # JSON temporaire
â”‚
â”œâ”€â”€ ğŸ““ notebooks/                   # Analyses Jupyter (legacy)
â”‚   â””â”€â”€ analysis.ipynb              # Notebook d'analyse
â”‚
â”œâ”€â”€ ğŸ“ logs/                        # Logs Airflow (ignorÃ© Git)
â”‚
â”œâ”€â”€ ğŸ§ª tests/                       # Tests unitaires (Ã  venir)
â”‚   â””â”€â”€ .gitkeep
â”‚
â””â”€â”€ ğŸ“¦ archive/                     # Anciennes versions
    â”œâ”€â”€ Adzuna API/                 # Ancienne structure
    â””â”€â”€ France Travail API/         # Ancienne API
```

---

## ğŸ—„ï¸ SchÃ©mas PostgreSQL - Structure dÃ©taillÃ©e

```
Database: jobmarket
â”‚
â”œâ”€â”€ Schema: raw (DonnÃ©es brutes)
â”‚   â”‚
â”‚   â”œâ”€â”€ Table: jobs_raw
â”‚   â”‚   â”œâ”€â”€ id (PK) â†’ INTEGER
â”‚   â”‚   â”œâ”€â”€ job_id â†’ VARCHAR(50) UNIQUE
â”‚   â”‚   â”œâ”€â”€ data â†’ JSONB â˜… (JSON complet de l'API)
â”‚   â”‚   â”œâ”€â”€ source â†’ VARCHAR(50) ('adzuna')
â”‚   â”‚   â”œâ”€â”€ created_at â†’ TIMESTAMP
â”‚   â”‚   â””â”€â”€ updated_at â†’ TIMESTAMP
â”‚   â”‚
â”‚   â””â”€â”€ Table: import_metadata
â”‚       â”œâ”€â”€ id (PK) â†’ SERIAL
â”‚       â”œâ”€â”€ search_term â†’ VARCHAR(100)
â”‚       â”œâ”€â”€ total_jobs â†’ INTEGER
â”‚       â”œâ”€â”€ scraping_date â†’ TIMESTAMP
â”‚       â””â”€â”€ api_source â†’ VARCHAR(50)
â”‚
â”œâ”€â”€ Schema: staging (DonnÃ©es aplaties)
â”‚   â”‚
â”‚   â””â”€â”€ Table: jobs_flattened
â”‚       â”œâ”€â”€ job_id (PK) â†’ VARCHAR(50)
â”‚       â”œâ”€â”€ title â†’ TEXT
â”‚       â”œâ”€â”€ description â†’ TEXT
â”‚       â”œâ”€â”€ created â†’ TIMESTAMP
â”‚       â”œâ”€â”€ contract_type â†’ VARCHAR(50)
â”‚       â”œâ”€â”€ contract_time â†’ VARCHAR(50)
â”‚       â”œâ”€â”€ salary_min â†’ NUMERIC(10,2)
â”‚       â”œâ”€â”€ salary_max â†’ NUMERIC(10,2)
â”‚       â”œâ”€â”€ salary_is_predicted â†’ VARCHAR(10)
â”‚       â”œâ”€â”€ latitude â†’ NUMERIC(10,6)
â”‚       â”œâ”€â”€ longitude â†’ NUMERIC(10,6)
â”‚       â”œâ”€â”€ location_display â†’ VARCHAR(255)
â”‚       â”œâ”€â”€ country â†’ VARCHAR(100)
â”‚       â”œâ”€â”€ region â†’ VARCHAR(100)
â”‚       â”œâ”€â”€ department â†’ VARCHAR(100)
â”‚       â”œâ”€â”€ city â†’ VARCHAR(100)
â”‚       â”œâ”€â”€ company_name â†’ VARCHAR(255)
â”‚       â”œâ”€â”€ category_label â†’ VARCHAR(255)
â”‚       â”œâ”€â”€ category_tag â†’ VARCHAR(100)
â”‚       â”œâ”€â”€ redirect_url â†’ TEXT
â”‚       â”œâ”€â”€ raw_id (FK) â†’ INTEGER
â”‚       â””â”€â”€ processed_at â†’ TIMESTAMP
â”‚
â””â”€â”€ Schema: analytics (DonnÃ©es enrichies)
    â”‚
    â”œâ”€â”€ Table: jobs_clean
    â”‚   â”œâ”€â”€ job_id (PK) â†’ VARCHAR(50)
    â”‚   â”œâ”€â”€ [Toutes colonnes de staging]
    â”‚   â”œâ”€â”€ salary_avg â†’ NUMERIC(10,2) â˜… CalculÃ©
    â”‚   â”œâ”€â”€ salary_min_k â†’ NUMERIC(10,2) â˜… /1000
    â”‚   â”œâ”€â”€ salary_max_k â†’ NUMERIC(10,2) â˜… /1000
    â”‚   â”œâ”€â”€ salary_avg_k â†’ NUMERIC(10,2) â˜… /1000
    â”‚   â”œâ”€â”€ is_paris â†’ BOOLEAN â˜… Flag
    â”‚   â”œâ”€â”€ is_ile_de_france â†’ BOOLEAN â˜… Flag
    â”‚   â”œâ”€â”€ is_data_scientist â†’ BOOLEAN â˜… Flag
    â”‚   â”œâ”€â”€ is_data_analyst â†’ BOOLEAN â˜… Flag
    â”‚   â”œâ”€â”€ is_data_engineer â†’ BOOLEAN â˜… Flag
    â”‚   â”œâ”€â”€ is_alternance â†’ BOOLEAN â˜… Flag
    â”‚   â”œâ”€â”€ year â†’ INTEGER â˜… Extrait
    â”‚   â”œâ”€â”€ month â†’ INTEGER â˜… Extrait
    â”‚   â”œâ”€â”€ year_month â†’ VARCHAR(7) â˜… 'YYYY-MM'
    â”‚   â”œâ”€â”€ description_length â†’ INTEGER â˜… CalculÃ©
    â”‚   â”œâ”€â”€ staging_id â†’ VARCHAR(50)
    â”‚   â””â”€â”€ created_at â†’ TIMESTAMP
    â”‚
    â””â”€â”€ Vues (Views)
        â”œâ”€â”€ vw_salaries_by_job       â†’ Salaires moyens
        â”œâ”€â”€ vw_top_companies         â†’ Top entreprises
        â”œâ”€â”€ vw_geo_distribution      â†’ Distribution gÃ©o
        â”œâ”€â”€ vw_monthly_trends        â†’ Tendances mensuelles
        â””â”€â”€ vw_top_cities            â†’ Top villes
```

---

## ğŸ” SÃ©curitÃ© & Gestion des secrets

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   GESTION DES SECRETS                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Fichiers sensibles (NON versionnÃ©s dans Git):
  â”œâ”€â”€ src/config.json              â† ClÃ©s API Adzuna
  â”œâ”€â”€ data/jobs_data.json          â† DonnÃ©es scrapÃ©es
  â”œâ”€â”€ logs/*                       â† Logs Airflow
  â””â”€â”€ .pgdata/                     â† DonnÃ©es PostgreSQL

Fichiers versionnÃ©s (templates):
  â””â”€â”€ src/config.example.adzuna.json  â† Template pour config.json

Variables d'environnement (docker-compose.yml):
  PostgreSQL:
    â€¢ POSTGRES_USER: jobmarket_user
    â€¢ POSTGRES_PASSWORD: jobmarket_pass
    â€¢ POSTGRES_DB: jobmarket
  
  Airflow:
    â€¢ AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql://airflow:airflow@...
    â€¢ AIRFLOW__CORE__FERNET_KEY: (clÃ© de chiffrement)
    â€¢ AIRFLOW__WEBSERVER__SECRET_KEY: (clÃ© de session)
    â€¢ JOBMARKET_DB_* (connexion Ã  la DB mÃ©tier)
```

---

## ğŸš€ Flux d'exÃ©cution - Chronologie

```
DÃ‰MARRAGE DU PROJET
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Installation initiale
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   $ git clone <repo>
   $ cd JobMarket
   $ python -m venv venv
   $ venv\Scripts\activate
   $ pip install -r requirements.txt
   $ cp src/config.example.adzuna.json src/config.json
   $ # Ã‰diter src/config.json avec vos clÃ©s API

2. DÃ©marrage Docker
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   $ docker-compose up -d
   
   â†’ PostgreSQL dÃ©marre
     â”œâ”€ ExÃ©cute sql/init/* dans l'ordre
     â”œâ”€ CrÃ©e base "airflow" + user "airflow"
     â”œâ”€ CrÃ©e base "jobmarket"
     â”œâ”€ CrÃ©e schemas: raw, staging, analytics
     â”œâ”€ CrÃ©e tables + index
     â””â”€ CrÃ©e vues analytiques
   
   â†’ Airflow dÃ©marre (dÃ©pend de PostgreSQL)
     â”œâ”€ Init DB dans base "airflow"
     â”œâ”€ CrÃ©e user admin/admin
     â”œâ”€ DÃ©marre Webserver (port 8080)
     â””â”€ DÃ©marre Scheduler

3. Configuration Airflow (une seule fois)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   http://localhost:8080 (admin/admin)
   
   â†’ Admin â†’ Connections â†’ Add
     Connection ID: jobmarket_postgres
     Type: Postgres
     Host: postgres
     Schema: jobmarket
     Login: jobmarket_user
     Password: jobmarket_pass
     Port: 5432

4. ExÃ©cution du pipeline (mode TEST par dÃ©faut)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â†’ Activer le DAG "jobmarket_etl_pipeline"
   â†’ Cliquer "Trigger DAG"
   
   Ã‰tapes (mode TEST - 5 pages):
   [00:00] â–º start_pipeline
   [00:01] â–º scrape_adzuna (1-2 min)
           â””â”€ Scrape 5 pages (~100 offres)
           â””â”€ Sauvegarde data/jobs_data.json
   [02:00] â–º load_to_postgres (10-30s)
           â””â”€ Charge JSON dans raw.jobs_raw
   [02:30] â–º transform_to_staging (5-10s)
           â””â”€ Aplatit JSON en colonnes SQL
   [02:40] â–º transform_to_analytics (5-10s)
           â””â”€ Enrichit avec calculs et flags
   [02:50] â–º verify_pipeline (2-5s)
           â””â”€ VÃ©rifie + affiche statistiques
   [02:55] â–º end_pipeline
   [02:55] âœ… PIPELINE TERMINÃ‰

5. Analyse des rÃ©sultats
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â†’ DBeaver: Connexion Ã  localhost:5432
   â†’ RequÃªtes SQL sur analytics.*
   â†’ Utilisation des vues prÃ©-calculÃ©es

PASSAGE EN PRODUCTION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Passer en mode PRODUCTION
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Airflow â†’ Admin â†’ Variables â†’ Add
   Key: scraping_mode
   Val: production

2. Planification automatique (optionnel)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Modifier dags/jobmarket_etl_pipeline.py:
   schedule_interval='0 6 * * *'  # Tous les jours Ã  6h
   
   $ docker-compose restart airflow

3. Monitoring
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â†’ Airflow UI: VÃ©rifier les runs
   â†’ DBeaver: VÃ©rifier la qualitÃ© des donnÃ©es
   â†’ Logs: docker-compose logs -f airflow
```

---

## ğŸ“Š VolumÃ©trie & Performances

```
MODE TEST (5 pages)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DonnÃ©es:           ~100 offres
DurÃ©e totale:      2-3 minutes
  â”œâ”€ Scraping:     1-2 min
  â”œâ”€ Load:         10-30s
  â”œâ”€ Transform:    10-20s
  â””â”€ Verify:       2-5s

Taille fichier:    ~200 KB (JSON)
Taille DB:         ~50 KB (PostgreSQL)


MODE PRODUCTION (700 pages)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DonnÃ©es:           ~14 000 offres
DurÃ©e totale:      30-35 minutes
  â”œâ”€ Scraping:     25-30 min
  â”œâ”€ Load:         30-60s
  â”œâ”€ Transform:    10-30s
  â””â”€ Verify:       5-10s

Taille fichier:    ~30 MB (JSON)
Taille DB:         ~15 MB (PostgreSQL)
  â”œâ”€ raw:          ~10 MB (JSONB)
  â”œâ”€ staging:      ~3 MB (colonnes)
  â””â”€ analytics:    ~2 MB (enrichi)
```

---

## ğŸ¯ Points clÃ©s de l'architecture

### âœ… Avantages

1. **ModularitÃ©** : Chaque composant a un rÃ´le prÃ©cis
2. **ScalabilitÃ©** : Docker facilite le dÃ©ploiement
3. **MaintenabilitÃ©** : Code organisÃ©, bien documentÃ©
4. **FlexibilitÃ©** : Mode TEST/PROD configurables
5. **TraÃ§abilitÃ©** : Logs Airflow + mÃ©tadonnÃ©es DB
6. **RÃ©silience** : Retry automatique, gestion erreurs
7. **Performance** : Index SQL, batch insert

### ğŸ“ˆ Ã‰volutions futures possibles

1. **Airflow distribuÃ©** : CeleryExecutor pour parallÃ©lisation
2. **Cache Redis** : AmÃ©liorer performances Airflow
3. **Data quality** : Great Expectations pour validation
4. **CI/CD** : GitHub Actions pour tests automatiques
5. **Monitoring** : Prometheus + Grafana
6. **BI Tool** : Metabase ou Superset pour dashboards
7. **ML Pipeline** : PrÃ©diction salaires, classification

---

## ğŸ“š Liens vers la documentation

### Documentation principale
- [README.md](README.md) - Vue d'ensemble du projet
- [DECISIONS.md](DECISIONS.md) - DÃ©cisions techniques

### Guides dÃ©taillÃ©s (docs/)
- [docs/AIRFLOW_SETUP.md](docs/AIRFLOW_SETUP.md) - Configuration Airflow
- [docs/AIRFLOW_VARIABLES.md](docs/AIRFLOW_VARIABLES.md) - Modes TEST/PRODUCTION
- [docs/DATABASE_SETUP.md](docs/DATABASE_SETUP.md) - Configuration PostgreSQL
- [docs/DBEAVER_SETUP.md](docs/DBEAVER_SETUP.md) - Configuration DBeaver

---

**ğŸ“ Note** : Ce document est mis Ã  jour rÃ©guliÃ¨rement. Version: 1.0 (DÃ©cembre 2025)

