"""
DAG Airflow : Pipeline ETL JobMarket

Ce DAG orchestre le pipeline complet :
1. Scraping des offres d'emploi depuis Adzuna
2. Chargement dans PostgreSQL (RAW)
3. Transformation vers STAGING
4. Transformation vers ANALYTICS
5. VÃ©rification finale

Auteur: JobMarket Project
"""

from datetime import datetime, timedelta
from pathlib import Path
import json
import sys

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.operators.dummy import DummyOperator

# Ajouter le dossier src au path pour imports
sys.path.insert(0, '/opt/airflow/src')

# Configuration par dÃ©faut du DAG
default_args = {
    'owner': 'jobmarket',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email': ['admin@jobmarket.local'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}


def scrape_adzuna_jobs(**context):
    """
    TÃ¢che 1: Scrape les offres d'emploi depuis l'API Adzuna
    """
    from scraper_adzuna import AdzunaJobScraper, load_config
    
    print("=" * 60)
    print("ğŸ” Ã‰TAPE 1: SCRAPING ADZUNA")
    print("=" * 60)
    
    # Charger la configuration
    config = load_config()
    if not config:
        raise ValueError("âŒ Fichier config.json introuvable ou invalide")
    
    adzuna_config = config.get('adzuna', {})
    app_id = adzuna_config.get('app_id')
    app_key = adzuna_config.get('app_key')
    
    if not app_id or not app_key:
        raise ValueError("âŒ ClÃ©s API Adzuna manquantes dans config.json")
    
    # ParamÃ¨tres de scraping
    search_term = "data"
    max_pages = 700  # ou None pour tout
    delay = 0.2
    
    # CrÃ©er le scraper et rÃ©cupÃ©rer les donnÃ©es
    print(f"ğŸ”„ Scraping en cours (terme: '{search_term}', max_pages: {max_pages})...")
    scraper = AdzunaJobScraper(app_id, app_key)
    
    all_jobs = scraper.scrape_all_jobs(
        search_term=search_term,
        max_pages=max_pages,
        delay=delay
    )
    
    if not all_jobs:
        raise ValueError("âŒ Aucune offre rÃ©cupÃ©rÃ©e")
    
    # Sauvegarder dans data/
    filepath = scraper.save_to_json(all_jobs, search_term=search_term)
    
    print(f"\nâœ… Scraping terminÃ© avec succÃ¨s!")
    print(f"   ğŸ“Š Nombre d'offres: {len(all_jobs)}")
    print(f"   ğŸ’¾ Fichier: {filepath}")
    
    # Passer le chemin du fichier Ã  la tÃ¢che suivante
    context['task_instance'].xcom_push(key='json_filepath', value=str(filepath))
    context['task_instance'].xcom_push(key='nb_jobs', value=len(all_jobs))
    
    return str(filepath)


def load_to_postgres(**context):
    """
    TÃ¢che 2: Charge le JSON dans PostgreSQL (raw schema)
    """
    from db_loader import JobMarketLoader
    
    print("=" * 60)
    print("ğŸ“¥ Ã‰TAPE 2: CHARGEMENT DANS POSTGRESQL")
    print("=" * 60)
    
    # RÃ©cupÃ©rer le chemin du fichier JSON de la tÃ¢che prÃ©cÃ©dente
    ti = context['task_instance']
    json_filepath = ti.xcom_pull(task_ids='scrape_adzuna', key='json_filepath')
    
    if not json_filepath:
        raise ValueError("âŒ Chemin du fichier JSON introuvable")
    
    json_path = Path(json_filepath)
    if not json_path.exists():
        raise FileNotFoundError(f"âŒ Fichier introuvable: {json_path}")
    
    print(f"ğŸ“‚ Fichier JSON: {json_path}")
    
    # Charger les donnÃ©es
    loader = JobMarketLoader()
    result = loader.load_from_json(json_path)
    
    if not result['success']:
        raise ValueError("âŒ Ã‰chec du chargement")
    
    print(f"\nâœ… Chargement terminÃ© avec succÃ¨s!")
    print(f"   ğŸ“Š Import ID: {result['import_id']}")
    print(f"   ğŸ“‹ Offres insÃ©rÃ©es: {result['nb_jobs_inserted']}")
    
    # Passer les stats Ã  la tÃ¢che suivante
    ti.xcom_push(key='import_id', value=result['import_id'])
    ti.xcom_push(key='nb_jobs_inserted', value=result['nb_jobs_inserted'])
    
    return result['nb_jobs_inserted']


def verify_pipeline(**context):
    """
    TÃ¢che finale: VÃ©rifie que le pipeline s'est bien exÃ©cutÃ©
    """
    print("=" * 60)
    print("âœ… VÃ‰RIFICATION FINALE")
    print("=" * 60)
    
    ti = context['task_instance']
    
    # RÃ©cupÃ©rer les stats des tÃ¢ches prÃ©cÃ©dentes
    nb_jobs_scraped = ti.xcom_pull(task_ids='scrape_adzuna', key='nb_jobs')
    nb_jobs_inserted = ti.xcom_pull(task_ids='load_to_postgres', key='nb_jobs_inserted')
    import_id = ti.xcom_pull(task_ids='load_to_postgres', key='import_id')
    
    # VÃ©rifier les tables via PostgreSQL
    hook = PostgresHook(postgres_conn_id='jobmarket_postgres')
    
    # Compter les lignes dans chaque table
    counts = {
        'raw': hook.get_first("SELECT COUNT(*) FROM raw.jobs_raw")[0],
        'staging': hook.get_first("SELECT COUNT(*) FROM staging.jobs_flattened")[0],
        'analytics': hook.get_first("SELECT COUNT(*) FROM analytics.jobs_clean")[0]
    }
    
    print("\nğŸ“Š STATISTIQUES DU PIPELINE:")
    print(f"   ğŸ” Offres scrapÃ©es: {nb_jobs_scraped}")
    print(f"   ğŸ“¥ Offres insÃ©rÃ©es (RAW): {nb_jobs_inserted}")
    print(f"   ğŸ—ƒï¸  Import ID: {import_id}")
    print("\nğŸ“‹ TABLES POSTGRESQL:")
    print(f"   â€¢ raw.jobs_raw: {counts['raw']} lignes")
    print(f"   â€¢ staging.jobs_flattened: {counts['staging']} lignes")
    print(f"   â€¢ analytics.jobs_clean: {counts['analytics']} lignes")
    
    # VÃ©rifications
    if counts['raw'] == 0:
        raise ValueError("âŒ Table raw.jobs_raw vide!")
    if counts['staging'] == 0:
        raise ValueError("âŒ Table staging.jobs_flattened vide!")
    if counts['analytics'] == 0:
        raise ValueError("âŒ Table analytics.jobs_clean vide!")
    
    print("\nâœ… PIPELINE TERMINÃ‰ AVEC SUCCÃˆS! ğŸ‰")
    print("=" * 60)
    
    return {
        'nb_jobs_scraped': nb_jobs_scraped,
        'nb_jobs_inserted': nb_jobs_inserted,
        'import_id': import_id,
        'counts': counts
    }


# DÃ©finition du DAG
with DAG(
    'jobmarket_etl_pipeline',
    default_args=default_args,
    description='Pipeline ETL complet pour JobMarket (Adzuna â†’ PostgreSQL)',
    schedule_interval=None,  # Manuel pour l'instant (mettre '@daily' pour quotidien)
    catchup=False,
    tags=['etl', 'jobmarket', 'adzuna', 'postgresql'],
) as dag:
    
    # TÃ¢che de dÃ©marrage (optionnelle, pour visualisation)
    start = DummyOperator(
        task_id='start_pipeline',
    )
    
    # TÃ¢che 1: Scraping Adzuna
    scrape_task = PythonOperator(
        task_id='scrape_adzuna',
        python_callable=scrape_adzuna_jobs,
        provide_context=True,
    )
    
    # TÃ¢che 2: Chargement RAW
    load_task = PythonOperator(
        task_id='load_to_postgres',
        python_callable=load_to_postgres,
        provide_context=True,
    )
    
    # TÃ¢che 3: Transformation STAGING
    transform_staging_task = PostgresOperator(
        task_id='transform_to_staging',
        postgres_conn_id='jobmarket_postgres',
        sql='transformations/01_load_staging.sql',
    )
    
    # TÃ¢che 4: Transformation ANALYTICS
    transform_analytics_task = PostgresOperator(
        task_id='transform_to_analytics',
        postgres_conn_id='jobmarket_postgres',
        sql='transformations/02_load_analytics.sql',
    )
    
    # TÃ¢che 5: VÃ©rification finale
    verify_task = PythonOperator(
        task_id='verify_pipeline',
        python_callable=verify_pipeline,
        provide_context=True,
    )
    
    # TÃ¢che de fin (optionnelle, pour visualisation)
    end = DummyOperator(
        task_id='end_pipeline',
    )
    
    # DÃ©finir l'ordre d'exÃ©cution (DAG)
    start >> scrape_task >> load_task >> transform_staging_task >> transform_analytics_task >> verify_task >> end

