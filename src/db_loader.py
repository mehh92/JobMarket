"""
Module de chargement des donn√©es JSON dans PostgreSQL
"""

import json
import psycopg2
from psycopg2.extras import execute_batch
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import logging

from db_config import get_db_config

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class JobMarketLoader:
    """
    Classe pour charger les donn√©es d'offres d'emploi dans PostgreSQL
    """
    
    def __init__(self, db_config: Optional[Dict] = None):
        """
        Initialise le loader avec la configuration DB
        
        Args:
            db_config: Configuration DB (si None, utilise get_db_config())
        """
        self.db_config = db_config or get_db_config()
        self.conn = None
        self.cur = None
        
    def connect(self):
        """√âtablit la connexion √† PostgreSQL"""
        try:
            logger.info(f"üîÑ Connexion √† PostgreSQL : {self.db_config['host']}:{self.db_config['port']}/{self.db_config['database']}")
            self.conn = psycopg2.connect(**self.db_config)
            self.cur = self.conn.cursor()
            logger.info("‚úÖ Connexion √©tablie")
        except psycopg2.Error as e:
            logger.error(f"‚ùå Erreur de connexion : {e}")
            raise
    
    def close(self):
        """Ferme la connexion"""
        if self.cur:
            self.cur.close()
        if self.conn:
            self.conn.close()
        logger.info("üîí Connexion ferm√©e")
    
    def load_json_file(self, json_path: Path) -> Dict:
        """
        Charge le fichier JSON
        
        Args:
            json_path: Chemin vers le fichier JSON
            
        Returns:
            Dict contenant metadata et jobs
        """
        logger.info(f"üìÇ Chargement du fichier : {json_path}")
        
        if not json_path.exists():
            raise FileNotFoundError(f"Fichier introuvable : {json_path}")
        
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        nb_jobs = len(data.get('jobs', []))
        logger.info(f"‚úÖ {nb_jobs} offres charg√©es depuis le JSON")
        
        return data
    
    def insert_metadata(self, metadata: Dict) -> int:
        """
        Ins√®re les m√©tadonn√©es de l'import
        
        Args:
            metadata: Dict contenant search_term, total_jobs, etc.
            
        Returns:
            ID de l'import cr√©√©
        """
        query = """
            INSERT INTO raw.import_metadata (search_term, total_jobs, scraping_date, api_source)
            VALUES (%s, %s, %s, %s)
            RETURNING id;
        """
        
        scraping_date = datetime.fromisoformat(metadata.get('scraping_date', datetime.now().isoformat()))
        
        self.cur.execute(query, (
            metadata.get('search_term'),
            metadata.get('total_jobs'),
            scraping_date,
            metadata.get('api_source', 'Adzuna')
        ))
        
        import_id = self.cur.fetchone()[0]
        logger.info(f"‚úÖ M√©tadonn√©es ins√©r√©es (import_id: {import_id})")
        
        return import_id
    
    def insert_jobs(self, jobs: List[Dict]) -> int:
        """
        Ins√®re les offres d'emploi dans raw.jobs_raw
        
        Args:
            jobs: Liste des offres d'emploi
            
        Returns:
            Nombre d'offres ins√©r√©es
        """
        if not jobs:
            logger.warning("‚ö†Ô∏è Aucune offre √† ins√©rer")
            return 0
        
        logger.info(f"üîÑ Insertion de {len(jobs)} offres...")
        
        # Pr√©parer les donn√©es pour l'insertion batch
        insert_query = """
            INSERT INTO raw.jobs_raw (job_id, data, source)
            VALUES (%s, %s, %s)
            ON CONFLICT (job_id) 
            DO UPDATE SET 
                data = EXCLUDED.data,
                updated_at = CURRENT_TIMESTAMP;
        """
        
        batch_data = [
            (
                job.get('id'),
                json.dumps(job),
                'adzuna'
            )
            for job in jobs
        ]
        
        # Insertion par batch de 1000
        execute_batch(self.cur, insert_query, batch_data, page_size=1000)
        
        logger.info(f"‚úÖ {len(batch_data)} offres ins√©r√©es/mises √† jour dans raw.jobs_raw")
        
        return len(batch_data)
    
    def load_from_json(self, json_path: Path) -> Dict[str, int]:
        """
        Charge un fichier JSON complet dans PostgreSQL
        
        Args:
            json_path: Chemin vers le fichier JSON
            
        Returns:
            Dict avec statistiques (import_id, nb_jobs_inserted)
        """
        try:
            # Charger le JSON
            data = self.load_json_file(json_path)
            
            # Connexion DB
            self.connect()
            
            # Ins√©rer les m√©tadonn√©es
            import_id = self.insert_metadata(data['metadata'])
            
            # Ins√©rer les jobs
            nb_inserted = self.insert_jobs(data['jobs'])
            
            # Commit
            self.conn.commit()
            
            logger.info(f"‚úÖ Import termin√© avec succ√®s !")
            logger.info(f"   üìä Import ID: {import_id}")
            logger.info(f"   üìã Offres ins√©r√©es: {nb_inserted}")
            
            return {
                'import_id': import_id,
                'nb_jobs_inserted': nb_inserted,
                'success': True
            }
            
        except Exception as e:
            if self.conn:
                self.conn.rollback()
            logger.error(f"‚ùå Erreur lors de l'import : {e}")
            raise
        
        finally:
            self.close()


def main():
    """
    Fonction principale pour tester le loader
    """
    # Chemin vers le fichier JSON (relatif au script)
    project_root = Path(__file__).parent.parent
    json_path = project_root / "data" / "jobs_data.json"
    
    logger.info("=" * 60)
    logger.info("üöÄ CHARGEMENT DES DONN√âES DANS POSTGRESQL")
    logger.info("=" * 60)
    
    # Cr√©er le loader et charger les donn√©es
    loader = JobMarketLoader()
    result = loader.load_from_json(json_path)
    
    if result['success']:
        logger.info("\n" + "=" * 60)
        logger.info("‚úÖ SUCC√àS - Donn√©es charg√©es dans PostgreSQL")
        logger.info("=" * 60)
        logger.info(f"üìä Statistiques :")
        logger.info(f"   ‚Ä¢ Import ID: {result['import_id']}")
        logger.info(f"   ‚Ä¢ Offres ins√©r√©es: {result['nb_jobs_inserted']}")
        logger.info("\nüí° Prochaine √©tape : Ex√©cuter les transformations SQL")
    else:
        logger.error("‚ùå √âCHEC - V√©rifiez les logs ci-dessus")


if __name__ == "__main__":
    main()

